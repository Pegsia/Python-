{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "#准备数据\r\n",
    "def loadDataSet():\r\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\r\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\r\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\r\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\r\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\r\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\r\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\r\n",
    "    return postingList,classVec\r\n",
    "\r\n",
    "# 对词语取并集\r\n",
    "def creatVoca(dataset):\r\n",
    "    myVoca = set()\r\n",
    "    for doc in dataset:\r\n",
    "        myVoca |= set(doc)\r\n",
    "    return list(myVoca)\r\n",
    "\r\n",
    "def wordSetVec(vocaList, inData):\r\n",
    "    wordV = [0] * len(vocaList)\r\n",
    "    for word in inData:\r\n",
    "        if word in vocaList:\r\n",
    "            wordV[vocaList.index(word)] = 1 \r\n",
    "    return wordV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算p(ci), p(wi|c1), p(wi|c0)\r\n",
    "def trainNB(wordVec, classVec):\r\n",
    "    classNum = len(classVec)  # 类的总数\r\n",
    "    wordNum  = len(wordVec[0])# 词的总数\r\n",
    "    n = len(wordVec)\r\n",
    "    pc = np.sum(classVec) / classNum # p(c1)\r\n",
    "    # 最后总p(wi|ci)是乘算，如果一个为0，则总数为0，故将初始值设为1，总数设为2\r\n",
    "    pc1Num = np.ones(wordNum) # p(wi|c1)中的单词出现次数\r\n",
    "    pc1Sum = 2             #                    总数\r\n",
    "    pc0Num = np.ones(wordNum)\r\n",
    "    pc0Sum = 2\r\n",
    "    for i in range(n):\r\n",
    "        if classVec[i] == 1:\r\n",
    "            pc1Num += wordVec[i]\r\n",
    "            pc1Sum += np.sum(wordVec[i])\r\n",
    "        else:\r\n",
    "            pc0Num += wordVec[i]\r\n",
    "            pc0Sum += np.sum(wordVec[i])\r\n",
    "    \r\n",
    "    # 防止下溢出，取log\r\n",
    "    pc1 = np.log(pc1Num / pc1Sum)\r\n",
    "    pc0 = np.log(pc0Num / pc0Sum)\r\n",
    "    return pc1, pc0, pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算概率，进行分类\r\n",
    "def classNB(wordSet, pc1, pc0, pc):\r\n",
    "    # p(wi|ci) * p(c)\r\n",
    "    p0 = np.sum(wordSet * pc0) + np.log(pc) # 前面已经取过对数，故和后面相加\r\n",
    "    p1 = np.sum(wordSet * pc1) + np.log(1 - pc)\r\n",
    "    print(p0, p1, sep = '\\n')\r\n",
    "    if p0 > p1:\r\n",
    "        return 0\r\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.694848072384611\n",
      "-9.826714493730215\n",
      "0\n",
      "[-2.35137526 -3.04452244 -2.35137526 -3.04452244 -2.35137526 -2.35137526\n",
      " -2.35137526 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -2.35137526\n",
      " -3.04452244 -2.35137526 -2.35137526 -2.35137526 -3.04452244 -3.04452244\n",
      " -1.94591015 -1.65822808 -2.35137526 -3.04452244 -3.04452244 -1.94591015\n",
      " -3.04452244 -3.04452244 -3.04452244 -2.35137526 -3.04452244 -3.04452244\n",
      " -3.04452244 -3.04452244]\n",
      "[-3.25809654 -2.56494936 -3.25809654 -2.56494936 -2.56494936 -3.25809654\n",
      " -3.25809654 -2.56494936 -1.87180218 -3.25809654 -2.56494936 -3.25809654\n",
      " -2.56494936 -3.25809654 -3.25809654 -2.15948425 -2.56494936 -2.56494936\n",
      " -3.25809654 -3.25809654 -2.56494936 -2.56494936 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936 -2.56494936 -3.25809654 -2.56494936 -2.56494936\n",
      " -2.56494936 -2.56494936]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def testNB():\r\n",
    "    wordList, classList = loadDataSet()\r\n",
    "    voca = creatVoca(wordList)\r\n",
    "    wordSet = []\r\n",
    "    for doc in wordList:\r\n",
    "        wordSet.append(wordSetVec(voca, doc))\r\n",
    "    \r\n",
    "    pc1, pc0, pc = trainNB(wordSet, classList)\r\n",
    "    testentry = ['love', 'my', 'dalmation']\r\n",
    "    thisDoc = wordSetVec(voca, testentry)\r\n",
    "    print(classNB(thisDoc, pc1, pc0, pc))\r\n",
    "    print(pc1, pc0, pc, sep = '\\n')\r\n",
    "\r\n",
    "testNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e5edc721c267c354f7c3a43761a84baa24063647031c2095a95e3fbc9f4e28d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}